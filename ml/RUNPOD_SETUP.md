# RunPod Training Setup Guide

## Overview

This guide helps you set up GPU-based fine-tuning on RunPod for the DevAI project. RunPod provides affordable GPU cloud instances perfect for model training.

## Prerequisites

- RunPod account with API key set in environment
- Training data exported from DevAI backend
- This repository's ML files

## Quick Start

### 1. Export Training Data

```bash
cd ml/
python3 scripts/training_manager.py --token [TEAM_TOKEN] --min-pairs 50 train
```

### 2. Create RunPod Instance

1. Go to https://runpod.io/console/pods
2. Choose template: "PyTorch 2.0.1" or "RunPod PyTorch"
3. GPU: Select RTX A4000 or better (RTX 4090 recommended)
4. Volume: 20GB+ persistent storage
5. Enable SSH access
6. Start instance

### 3. Upload Files to RunPod

Upload these files to `/workspace/` on your RunPod instance:

- `training_data_[job-id].json` (generated by training manager)
- `scripts/train.py`
- `requirements.txt`
- `setup_runpod.sh`

### 4. Run Training Setup

```bash
# SSH into your RunPod instance
cd /workspace
chmod +x setup_runpod.sh
./setup_runpod.sh
```

### 5. Start Training

```bash
python3 train.py \
  --data training_data_[job-id].json \
  --output ./models/fine-tuned/devai_[job-id] \
  --epochs 3 \
  --batch_size 4
```

### 6. Monitor Training

```bash
# Monitor GPU usage
nvidia-smi

# Monitor training logs
tail -f ./models/fine-tuned/devai_[job-id]/logs/events.out.tfevents.*

# Check training progress
tensorboard --logdir=./models/fine-tuned/devai_[job-id]/logs --host=0.0.0.0 --port=6006
```

### 7. Download Trained Model

- Use RunPod's web interface download feature
- Download the entire `./models/fine-tuned/devai_[job-id]/` folder
- Or use `rsync` with SSH

## RunPod Instance Recommendations

### GPU Options (by budget):

- **Budget**: RTX A4000 (16GB VRAM) - ~$0.34/hour
- **Recommended**: RTX 4090 (24GB VRAM) - ~$0.83/hour
- **High-end**: A100 (40GB VRAM) - ~$1.89/hour

### Training Time Estimates:

- **50 pairs**: 15-30 minutes
- **200 pairs**: 45-90 minutes
- **500+ pairs**: 2-4 hours

### Cost Estimates:

- **Small training (50 pairs)**: $0.17 - $0.42
- **Medium training (200 pairs)**: $0.38 - $1.25
- **Large training (500+ pairs)**: $1.66 - $7.56

## Automated Training Commands

### Test with Sample Data

```bash
# Generate sample data and train with low threshold
python3 scripts/training_manager.py --token [TEAM_TOKEN] --min-pairs 50 train
```

### Production Training

```bash
# Full workflow when ready (200+ pairs)
python3 scripts/training_manager.py --token [TEAM_TOKEN] workflow --auto
```

### Manual Control

```bash
# Check readiness
python3 scripts/training_manager.py --token [TEAM_TOKEN] check

# Export data only
python3 scripts/training_manager.py --token [TEAM_TOKEN] export

# Trigger training job record
python3 scripts/training_manager.py --token [TEAM_TOKEN] trigger
```

## File Upload Methods

### Method 1: RunPod Web Interface

1. Go to your pod's file manager
2. Navigate to `/workspace/`
3. Upload files via drag-and-drop

### Method 2: SSH/SCP

```bash
# Get SSH details from RunPod console
scp -P [SSH_PORT] training_data.json root@[POD_IP]:/workspace/
scp -P [SSH_PORT] scripts/* root@[POD_IP]:/workspace/
scp -P [SSH_PORT] requirements.txt root@[POD_IP]:/workspace/
```

### Method 3: Git Clone (if pushing files)

```bash
# On RunPod instance
cd /workspace
git clone [your-repo-url]
cd [repo-name]/ml
```

## Training Output

### Expected Files After Training:

```
/workspace/models/fine-tuned/devai_[job-id]/
├── adapter_config.json      # LoRA configuration
├── adapter_model.bin        # Fine-tuned weights
├── README.md               # Training summary
├── Modelfile              # Ollama deployment file
├── tokenizer.json         # Tokenizer files
├── tokenizer_config.json
└── logs/                  # TensorBoard logs
    └── events.out.tfevents.*
```

### Model Deployment Locally:

```bash
# After downloading the model folder
cd ml/models/fine-tuned/devai_[job-id]/
ollama create devai-assistant-[job-id] -f Modelfile

# Test the model
ollama run devai-assistant-[job-id] "How does authentication work in React?"
```

## Troubleshooting

### Common Issues:

**1. "CUDA out of memory"**

```bash
# Reduce batch size
python3 train.py --data training_data.json --output ./model --batch_size 2
```

**2. "Module not found"**

```bash
# Make sure requirements are installed
pip install -r requirements.txt
```

**3. "Training data not found"**

```bash
# Check file upload
ls -la /workspace/training_data*.json
```

**4. "Permission denied"**

```bash
# Fix file permissions
chmod +x setup_runpod.sh
chmod 644 *.json
```

### Monitoring Commands:

```bash
# Check GPU memory usage
nvidia-smi

# Monitor training progress
watch -n 1 nvidia-smi

# Check disk space
df -h

# Monitor training logs
tail -f ./models/fine-tuned/*/logs/events.out.tfevents.*
```

## Cost Optimization

### Tips to Reduce Costs:

1. **Use smaller models for testing** - Start with 50 pairs
2. **Choose appropriate GPU** - RTX A4000 for most cases
3. **Stop instances when done** - Don't leave running
4. **Use spot instances** - Up to 70% cost savings
5. **Batch multiple training runs** - Upload multiple datasets

### Monitoring Spend:

- Check RunPod billing dashboard regularly
- Set spending alerts in RunPod console
- Use `nvidia-smi` to ensure GPU utilization

## Security Notes

- API keys and tokens are configured in environment variables
- Never commit sensitive credentials to version control
- Use secure SSH keys for RunPod access
- Delete training data from instances after use

## Next Steps

After successful training:

1. Download the fine-tuned model
2. Deploy with Ollama locally
3. Update your DevAI RAG service to use the new model
4. Test the improved responses
5. Schedule regular retraining as more data accumulates
